{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting vs. overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Underfitting is error due to bias\n",
    "- Overfitting is error due to variance\n",
    "- It's really hard to find the right architecture for a neural network (balance between being overly simplistic vs. overly complicated). Best to err on the side of overly complicated models and introduce controls to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques to control overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model complexity graph plots the test and training error versus the number of epochs, which can be considered as a measure of complexity.\n",
    "- When considering errors on trainings vs. test data on the model complexity graph, the error will decrease for the training set with more epochs.\n",
    "- The error on the test data is initially large due to the randomised weights, but then decreases as long as the model generalises well until it reaches the minimum point - the Goldilocks spot. After this point, the model starts overfitting and starts memorizing the training data.\n",
    "- **Early stopping** determines the number of epochs required to reach the Goldilocks point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Large weights cause the sigmoid to be very steep, such that the gradients will generally be close to zero. The model is too certain about his predictions. Also, if this model misclassifies, then it will produce large errors. This makes it hard to do gradient descent.\n",
    "- However, if a model with large weights classifies correctly, it will produce small error. Therefore, in order to incentivize the procedure to prefer smaller weights, you need to penalize large coefficients. You need to add terms to the error function which grow large for large weights:\n",
    "    - L1: Add sum of absolute values of weights (times a constant lambda)\n",
    "        - Better for feature selection as it produces sparse vectors.\n",
    "    - L2: Add sum of squared values of weights (times a constant lambda)\n",
    "        - Normally better for training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What you see a lot when training neural networks is that one part of the network has large weights and dominates all the training. This whereas other parts have smaller weights and don't play a large part in training.\n",
    "- To prevent this, in each epoch you can randomnly remove a node or nodes from the network to engage other nodes in training. This method is called dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random restart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random restart is a way of ending up in a local minimum rather than the global minimum.\n",
    "- You randomnly start gradient descent from different starting points and see which track leads to the lowest error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The derivative of the sigmoid is very flat at both ends of the function. Therefore, the gradients are very low and lead to slow learning. This is further compounded with more layers since backpropagation multiplies the error terms.\n",
    "- The best way to fix this is to change the activation function, e.g.:\n",
    "    - Hyperbolic tangent function\n",
    "    - Rectified Linear Unit (ReLU)\n",
    "- You can apply a variation of activation function across layers, dependent on the requirement of the output unit. E.g. if that still needs probabilities, you would still use a sigmoid for the output layer, but a different one for the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch vs stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far each, in each epoch we would feedforward/backward all data points through the network to update the weights a single time. This can be very computationally demanding.\n",
    "- Alternatively you can apply **stochastic gradient descent**:\n",
    "    - Take small subsets (batches) of data, run them through the network, calculate the gradient of the error function for this batch and continue with the following batch.\n",
    "    - To ensure all batches are considered, every epoch will have sub epochs, one for each batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lower learning rates are better than bigger ones. With bigger learning rates you have the chance that you overshould and keep going until forever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Momentum attempts to solve for the local minimum problem by increasing the gradient to hump out of the minimum.\n",
    "- Momentum determines the size of the next step by considering the sizes of the previous steps scaled by a parameter beta (between 0 and 1) to the power of the number of steps back so that the weight of a step decreasing if more time has passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
