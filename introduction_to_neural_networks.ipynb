{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single perceptron defines a linear equation (e.g. Wx + b = 0) and a decision function (step) \n",
    "- The weights of the linear equation label the edges \n",
    "- The bias term labels the node \n",
    "- Bias can also be considered as a weight through an edge \n",
    "- Step function converts score to binary decision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND, OR, NOT and XOR perceptons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AND**\n",
    "\n",
    "- Single layer perceptron \n",
    "- 1 if both inputs are 1, 0 otherwise \n",
    "\n",
    "OR \n",
    "- Single layer perceptron \n",
    "- 1 if both/either inputs are 1, 0 otherwise \n",
    "\n",
    "NOT \n",
    "- Single layer perceptron \n",
    "- Considers maximum 1 input and is 1 if input is 0, and vice versa \n",
    "\n",
    "NAND \n",
    "- Single layer perceptron: Combination of AND and NOT \n",
    "- Inverse of OR: 1 if both/either inputs are 0, 1 otherwise \n",
    "\n",
    "XOR \n",
    "- Two layer perceptron: \n",
    "    - 1st layer: 2 nodes NAND and OR \n",
    "    - 2nd layer: AND "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single line classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    # Loop through each observation\n",
    "    for i in range(len(X)):\n",
    "        # Get prediction\n",
    "        y_hat = prediction(X[i], W, b)\n",
    "        \n",
    "        # Determine the sign/value of the adjustment\n",
    "        delta_mult = 0\n",
    "        if y[i]-y_hat > 0:\n",
    "            delta_mult = 1\n",
    "        elif y[i]-y_hat < 0:\n",
    "            delta_mult = -1\n",
    "            \n",
    "        # Adjust the weights and bias after each prediction\n",
    "        W += delta_mult*learn_rate*X[i][:, np.newaxis]\n",
    "        b += delta_mult*learn_rate\n",
    "    \n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a measure of error of a classification problem, the error function must:\n",
    "- be continuous (to be differentiable)\n",
    "- penalize misclassifications strongly, and less for correctly classified points\n",
    "- be sort of a distance between the boundary line and the misclassified points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions must be converted from discrete to continuous through changing the activation function from a step function to the sigmoid function. The sigmoid function transform an unbounded input to a range bounded by 0 and 1. It transforms the discrete prediction into a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For converting scores in a multi-class classification problems to probabilities, we need a function which:\n",
    "- Produces probabilities for each class which add up to 1: Divide the class score by the sum of all scores.\n",
    "- Avoids a division by zero: Convert all scores to positive numbers\n",
    "- Assigns higher probabilities to higher scores: Use the exponential function to convert the scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T15:36:38.137298Z",
     "start_time": "2020-10-29T15:36:38.132063Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    exp_scores = np.exp(np.array(L))\n",
    "    sum_exp_scores = np.sum(exp_scores)\n",
    "    \n",
    "    return list(exp_scores/sum_exp_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizes the probability of observing the data given the model. It calculates the joint probability of the individual events occurring given the probability assigned to it by the model. For instance:\n",
    "- If an observation was a 1, what is the model's probability for this observation to be 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-entropy connects probabilities with error functions. It is the conversion of the joint probability into the sum of the negative of the logarithm of the probabilities:\n",
    "- A good model will provide a low cross-entropy (namely, a good model produces a high likelihood of the observations occurring given the data, and the negative of the logarithm of a high probability is a low number)\n",
    "- A bad model will provide a high cross-entropy\n",
    "\n",
    "Therefore, the cross-entropies can be considered as size of the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T16:08:40.884258Z",
     "start_time": "2020-10-29T16:08:40.870454Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    Y_array, P_array = np.array(Y), np.array(P)\n",
    "    \n",
    "    return -np.sum(Y_array * np.log(P_array) + (1-Y_array)*np.log(1-P_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uses as error function the cross-entropy divided by the number of observations. Since the model predictions are probabilities which are driven by the weights, the error function is a function of the weights.\n",
    "- Uses gradient descent to minimize the error function in a step-wise fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The gradient is formed by the vector of the partial derivative of the error function with respect to the weights and bias. It provides the **increase** in the error function for a 1-unit change in each of the weights.\n",
    "- The gradient is only valid on those particular values for the weights.\n",
    "- The gradient descent algorithm updates the weights in the opposite direction of the gradient (i.e. for decreases of the error function) scaled by the learning rate.\n",
    "- The gradients wrt the weights are simply the scalars times the values of the coordinate (or factor) related to that weight. This scalar is a multiple of the different between the label and the modelled probability. I.e. the farther away the probability is from the label, the higher the gradient (and vice versa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural networks come into play when the decision boundary to classify items is non-linear. \n",
    "- It effectively combines multiple linear decision boundaries into one model.\n",
    "- It combines the probability of each item as per each 'submodel', provides them a weight and produces a final probability for the entire network. The node which combines the two models, in itself becomes a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T17:38:56.478223Z",
     "start_time": "2020-10-29T17:38:56.473454Z"
    }
   },
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input layer**: \n",
    "    - First layer containing the inputs\n",
    "    - Number of **factors** determines the number of nodes\n",
    "- **Hidden layer**: \n",
    "    - Set of linear models created with the first layers\n",
    "    - Number of **models** determines the number of nodes\n",
    "- **Output layer**: \n",
    "    - Linear models get combined to obtain a non-linear model\n",
    "    - Number of **classes** determines the number of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep neural network** has multiple hidden layers:\n",
    "- First hidden layer create nonlinear models\n",
    "- Second hidden layer combines the nonlinear models to create more linear models.\n",
    "\n",
    "It splits the n-dimensional space (i.e. input layer) with a highly nonlinear boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward is the process of turning inputs into outputs through the layers of the network:\n",
    "- The first step is a matrix multiplication between the matrix of Weights (with each row consisting of the weights for each node in the hidden layer) and a vector of the input. Subsequently, the sigmoid function is applied to the resulting vector to produce probabilities.\n",
    "- The second step is a vector multiplication between the vector or Weights (with each weight corresponding to one edge going into the output layer) and the vector of probabilities resulting from the first step. Again the sigmoid function is applied to produce \n",
    "\n",
    "Therefore, the feedforward process applies a sequence of linear models and sigmoids to reach the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-29T18:05:58.023442Z",
     "start_time": "2020-10-29T18:05:58.015880Z"
    }
   },
   "source": [
    "In a nutshell, backpropagation will consist of:\n",
    "\n",
    "- Doing a feedforward operation.\n",
    "- Comparing the output of the model with the desired output.\n",
    "- Calculating the error.\n",
    "- Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.\n",
    "- Use this to update the weights, and get a better model.\n",
    "- Continue this until we have a model that is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down backpropagation in a multi-layered neural network:\n",
    "- The predictions are following from the feed-forward process (i.e. the sequence of matrix multiplications and sigmoids). Therefore, the predictions are a function of all the weights in the network. And therefore, the error function is also a function of all the weights in the network.\n",
    "- The gradient is a long vector of the partial derivatives of the error function w.r.t. to **all** the weights in the network.\n",
    "- Gradient descent will update each of the weights by adding the learning rate time the partial derivative of the error function w.r.t. that weight.\n",
    "- In determining the gradient, the process uses the chain rule to get the derivative of the composite functions (i.e. the sigmoids and matrix multiplications). The chain rule says that the derivative of a composite function is simply the multiplication of the partial derivatives. Therefore the partial derivatives for the weights are simply the multiplication of the partial derivatives (backwards through the network).\n",
    "\n",
    "Breaking down the layers of the partial derivatives:\n",
    "- First partial derivative: The partial derivative of the error function with respect to output layer is simply the difference between the label and the probability.\n",
    "- Second partial derivative: This probability in turn is derived from the output layer and is the sigmoid of a linear combination probabilities from the hidden layer. The partial derivative of this w.r.t. one of the weights is simply the level of the weight times the derivative of the sigmoid of the probability produced by the relevant node in the hidden layer. This is true since the derivative of the other sigmoid w.r.t. this weight is 0.\n",
    "- Third partial derivative: follows in similar fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
