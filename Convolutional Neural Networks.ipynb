{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A validation set is used to measure how well a model generalizes during training.\n",
    "- It can also tells us when to stop training; when the validation loss stops decreasing (and especially when the validation loss starts increasting and training loss is still decreasing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs vs MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNNs shine when exposed to real-world messy data.\n",
    "- CNNs use relationships between pixels that are close, whereas MLPs disregard these (spatial; 2D) relationships.\n",
    "- MLP only use fully connected layers. High model complexity. CNNs use more sparsely connected layers.\n",
    "- In CNNs, hidden nodes focuses on segments of the images (locally connected layers). Less prone to overfitting.\n",
    "- Multiple hidden nodes can focus on segments multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Frequency in images is a rate of change. Images change in space, and a high frequency image is one where the intensity changes a lot. And the level of brightness changes quickly from one pixel to the next. A low frequency image may be one that is relatively uniform in brightness or changes very slowly. \n",
    "- High-frequency components also correspond to the edges of objects in images, which can help us classify those objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-pass filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sharpen an image\n",
    "- Enhance high-frequency parts of an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A kernel is a matrix of numbers that modifies an image\n",
    "- Kernel convolution is an operation applied to the input image. It relies on centering a pixel and looking at it's surrounding neighbors. How to handle edges:\n",
    "    - Extend the image by copying border pixels far enough such that the filtered image has the same size as the original\n",
    "    - Pad the image with a border of 0's, i.e. black pixels\n",
    "    - Crop the image and skip the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Produced by applying a series of many different image filters, also known as convolutional kernels, to an input image.\n",
    "- 4 different filter produces 3 differently filtered output images. When we stack these images, we form a complete convolutional layer with a depth of 4.\n",
    "- NNs learn the weights of these filters.\n",
    "- Collections (stacks) of filtered outputs are called **feature maps** or **activation maps**.\n",
    "- Convolutional layers are only locally connected, i.e. connected to only a small subset of the previous layers' nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride and padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stride and padding are hyperparameters which control the behaviour of the convolutional layer.\n",
    "- **Stride** of the filter is the amount by which the filter slides over the image. Stride of 1 makes the filter output roughly the same size of the original image.\n",
    "- **Padding** includes a region (e.g. of pixel values 0) around the edge of the image in order to enable producing filter output values around the edges of the image.\n",
    "- **Window size** determines the size of the filter (i.e. kernel matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pooling layers take convolutional layers as an input, with the purpose of reducing dimensionality.\n",
    "- **Max pooling** takes a stack of feature maps as input, hovers over this map with a window with a certain window size and a certain stride and takes the maximum of each window. This reduces the size of the feature map. \n",
    "- **Average pooling** takes an average of the pixel values in the window, instead of the maximum. Max pooling is better at noticing the most important details about edges and other features in an image. In some cases, however, *smoothing* might be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capsule networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pooling operations throw away some image information. This can cause issues in applications where you do need spatial information, e.g. number of features and relative locations of these features (think of facial recognition and the distinguishing features).\n",
    "- **Capsule networks** provide a way to detection parts of an object and represent spatial relationships between those parts. They can recognize the same object in a variety of poses and with the typical number of features, even if they have not seen that pose in training data.\n",
    "- Capsule networks are made of parent and child nodes that build up a complete picture of an object. E.g. the face is the parent, and the individual features are the child nodes.\n",
    "- **Capsules** are collection of nodes, where each nodes contains information about a specific part like width, orientation, colors etcetera. Each capsules outputs a vector with some magnitude and orientation:\n",
    "    - **Magnitude**: probability that a part exists (i.e. the length of the vector). Should stay very high even when an object is in a different orientation. It is a normalised function of the weighted inputs to a particular capsule; a nonlinear function called **squashing**.\n",
    "    - **Orientation**: State of the part properties (i.e. the direction of the vector). It will only change if one of the part properties changes (e.g. position, orientation, shape).\n",
    "- The fact that the output of a capsule is vector, with some orientation, makes it possible to use a powerful **dynamic routing** mechanism to ensure that the output of a capsule gets sent to the appropriate parent calsuple in the next layer of capsules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Process for finding the *best* connections between the output of one capsule and the inputs of the next layer. \n",
    "- Imagine a capsule network for a face as a parent and fully connected subfeatures of the face (e.g. mouth&nose and eyes in the next layer, and each eye, nose mouth in the next). Dynamic routing iteratively changes *coupling coefficients*, which are the probabilities that the output of a capsule should go to a parent capsule. It does so by *routing by agreement* based on how the output vectors of the child versus the parameter are in agreement based on the dot product of these vectors.\n",
    "- This is useful for determining spatial relationships between the parts. Namely, the capsule network will check whether pose of each part (i.e. position and orientation) are in agreement based on the vector orientations of the child and parent capsules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layers in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN hyperparameters:\n",
    "- **Depth** of the *input* (e.g. 3 for RGB images)\n",
    "- **Depth** of the *convolutional layer* (e.g. 16 for 16 filters)\n",
    "- **Kernal size** of filters (e.g. 4 for 4x4 filters). Typically range between 3 to 7 for larger images\n",
    "- **Stride** for hovering. Typically 1 also being the default for many frameworks. Stride determines the size of the output of the layer. E.g. stride of 2 halves the size of the output.\n",
    "- **Padding** for the number of pixels to be padded around the edges of the image. Common methods of padding are padding with all 0-pixels or padding them with the nearest pixel value. There is a relation between the kernel size and layers of padding.\n",
    "\n",
    "Max pooling layers are typically put after convolutional layers to shrink the x,y dimensions of an input. But by applying more filters to each of the filter images of the previous convolutional layer, the next layer will become deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "        super(ModelName, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "              nn.Conv2d(1, 16, 2, stride=2),\n",
    "              nn.MaxPool2d(2, 2),\n",
    "              nn.ReLU(True),\n",
    "\n",
    "              nn.Conv2d(16, 32, 3, padding=1),\n",
    "              nn.MaxPool2d(2, 2),\n",
    "              nn.ReLU(True) \n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of parameters in a Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables:\n",
    "- **K**: Number of filters in CL\n",
    "- **F**: Height and width of the kernel/filter\n",
    "- **D_in**: Depth of the previous layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Per filter there is one weight per value in the filter, such that there are F\\*F\\*D_in weights per filter.\n",
    "- Across all filters there are then K\\*F\\*F\\*D_in weights in the CL\n",
    "- There is one bias term per filter.\n",
    "- Total number of parameters is then K\\*F\\*F\\*D_in + K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of the convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables:\n",
    "- **S** stride of the CL\n",
    "- **P** padding of the image\n",
    "- **W_in** width/height (square) of the previous layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depth is always equal to the number of filters K\n",
    "- Spatial dimensions of a CL can be calculated as: (W_in - F+2P)/S + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CL's in a CNN convert an image array to a representation that encodes only content of the image.\n",
    "- Feature vector <> Feature level representation of an image.\n",
    "- When an image moves through the CL's, more detail is discarded, like style and texture and the CNN is pushed towards answering key questions about the presence of unique patterns and shapes.\n",
    "- The featur vector is then feeding one or more fully connected layers to determine what object is contained in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invariant representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Algorithm should be able to detect whether an object is present irrespective of its size (**scale invariance**), rotation (**angle invariance**) or location in the image (**translation invariance**).\n",
    "- CNNs have some translation invariance due to max pooling applied after each convolutional layer. Max pooling extracts the signal irrespective of where that signal is coming from.\n",
    "- You can make the algorithm more invariant by including duplicates of samples but with objects at different angles and locations. These samples are seens as perceived by the model as different samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
